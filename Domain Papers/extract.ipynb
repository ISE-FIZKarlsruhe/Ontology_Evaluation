{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping MDPI as it does not contain all required .bib files.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bibtexparser\n",
    "from bibtexparser.bwriter import BibTexWriter\n",
    "from bibtexparser.bibdatabase import BibDatabase\n",
    "import random\n",
    "\n",
    "# Define the base directory containing the folders for each journal\n",
    "base_dir = '.'\n",
    "\n",
    "# Define criteria weights\n",
    "criteria_weights = {\n",
    "    'bibliographic': 0.2,\n",
    "    'relevance': 0.2,\n",
    "    'publication_date': 0.2,\n",
    "    'times_cited': 0.2,\n",
    "    'times_viewed': 0.2,\n",
    "}\n",
    "\n",
    "# Function to read bib files and return entries\n",
    "def read_bib_file(file_path):\n",
    "    with open(file_path, encoding='utf-8') as bibtex_file:\n",
    "        bib_database = bibtexparser.load(bibtex_file)\n",
    "        return bib_database.entries\n",
    "\n",
    "# Process each journal folder\n",
    "for journal_folder in os.listdir(base_dir):\n",
    "    journal_path = os.path.join(base_dir, journal_folder)\n",
    "    if os.path.isdir(journal_path):\n",
    "        # Read all entries from the provided bib files\n",
    "        bib_file_paths = [os.path.join(journal_path, f) for f in os.listdir(journal_path) if f.endswith('.bib')]\n",
    "\n",
    "        # Ensure we have all required criteria files\n",
    "        if len(bib_file_paths) != len(criteria_weights):\n",
    "            print(f\"Warning: Skipping {journal_folder} as it does not contain all required .bib files.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the number of samples to take from each criterion based on weights\n",
    "        total_samples = 20\n",
    "        samples_count = {criterion: int(total_samples * weight) for criterion, weight in criteria_weights.items()}\n",
    "\n",
    "        # Adjust for rounding issues to ensure the total number of samples is exactly total_samples\n",
    "        adjustment = total_samples - sum(samples_count.values())\n",
    "        most_weighted_criterion = max(criteria_weights, key=criteria_weights.get)\n",
    "        samples_count[most_weighted_criterion] += adjustment\n",
    "\n",
    "        # Sample from each criterion\n",
    "        selected_entries = []\n",
    "        for file_path, (criterion, count) in zip(bib_file_paths, samples_count.items()):\n",
    "            criterion_entries = read_bib_file(file_path)\n",
    "            # Sample entries ensuring we don't exceed available entries\n",
    "            if len(criterion_entries) < count:\n",
    "                count = len(criterion_entries)\n",
    "            selected_entries.extend(random.sample(criterion_entries, count))\n",
    "\n",
    "        # Remove duplicate entries by ID\n",
    "        unique_entries = {entry['ID']: entry for entry in selected_entries}.values()\n",
    "\n",
    "        # If we have less than total_samples unique entries due to duplicates, add more from the largest criterion\n",
    "        if len(unique_entries) < total_samples:\n",
    "            remaining_count = total_samples - len(unique_entries)\n",
    "            most_weighted_file_path = next(path for path in bib_file_paths if most_weighted_criterion in path)\n",
    "            remaining_entries = [entry for entry in read_bib_file(most_weighted_file_path) if entry['ID'] not in {entry['ID'] for entry in unique_entries}]\n",
    "            unique_entries = list(unique_entries) + random.sample(remaining_entries, min(remaining_count, len(remaining_entries)))\n",
    "\n",
    "        # Save the selected entries to a new .bib file\n",
    "        bib_db = BibDatabase()\n",
    "        bib_db.entries = unique_entries\n",
    "        writer = BibTexWriter()\n",
    "        abstracts_dir = os.path.join(journal_path, 'abstracts')\n",
    "        os.makedirs(abstracts_dir, exist_ok=True)\n",
    "        \n",
    "        selected_bib_path = os.path.join(abstracts_dir, 'selected_papers.bib')\n",
    "        \n",
    "        \n",
    "        with open(selected_bib_path, 'w', encoding='utf-8') as bibfile:\n",
    "            bibfile.write(writer.write(bib_db))\n",
    "\n",
    "        # Save each abstract to a separate .txt file\n",
    "        for entry in unique_entries:\n",
    "            abstract_file_path = os.path.join(abstracts_dir, f\"{entry['ID']}.txt\")\n",
    "            with open(abstract_file_path, 'w', encoding='utf-8') as abstract_file:\n",
    "                abstract_file.write(entry.get('abstract', 'No abstract available.'))\n",
    "\n",
    "print(\"Done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "372ad118f81acd4222add26895f13bc27a937b9d5021100e967f342fe93795e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
